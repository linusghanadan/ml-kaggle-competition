{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "931e1d19-1ce5-4691-98e3-49b5d0f58988",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "You will use the data contained in the train.csv file to train a model that will predict **dissolved inorganic carbon (DIC)** content in the water samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61266060-25da-43e3-ae3d-18179610ef67",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-20 13:34:48.200456: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-20 13:34:48.313773: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-20 13:34:48.317701: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/R/4.2.2/lib/R/lib:/lib:/usr/local/lib:/usr/lib/x86_64-linux-gnu:/usr/lib/jvm/java-11-openjdk-amd64/lib/server\n",
      "2024-03-20 13:34:48.317713: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2024-03-20 13:34:48.926062: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/R/4.2.2/lib/R/lib:/lib:/usr/local/lib:/usr/lib/x86_64-linux-gnu:/usr/lib/jvm/java-11-openjdk-amd64/lib/server\n",
      "2024-03-20 13:34:48.926154: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/R/4.2.2/lib/R/lib:/lib:/usr/local/lib:/usr/lib/x86_64-linux-gnu:/usr/lib/jvm/java-11-openjdk-amd64/lib/server\n",
      "2024-03-20 13:34:48.926160: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.regularizers import l1_l2\n",
    "from keras.optimizers import Adam\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d7fc6d5-b83e-4218-9ce3-dbdfb88fa3d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the environment variable to change the log level\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # 0 = default, 1 = no INFO, 2 = no INFO and WARNING, 3 = no INFO, WARNING, and ERROR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b400281-4d1f-4dde-9570-02d83b6e2a42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Turn off scientific notation\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "\n",
    "# Set seed\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd63faaa-3be6-4ad9-afb1-f181c2c01bce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1454 entries, 0 to 1453\n",
      "Data columns (total 19 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   id                 1454 non-null   int64  \n",
      " 1   lat_dec            1454 non-null   float64\n",
      " 2   lon_dec            1454 non-null   float64\n",
      " 3   no2um              1454 non-null   float64\n",
      " 4   no3um              1454 non-null   float64\n",
      " 5   nh3um              1454 non-null   float64\n",
      " 6   r_temp             1454 non-null   float64\n",
      " 7   r_depth            1454 non-null   int64  \n",
      " 8   r_sal              1454 non-null   float64\n",
      " 9   r_dynht            1454 non-null   float64\n",
      " 10  r_nuts             1454 non-null   float64\n",
      " 11  r_oxy_micromol.kg  1454 non-null   float64\n",
      " 12  unnamed:_12        0 non-null      float64\n",
      " 13  po4um              1454 non-null   float64\n",
      " 14  sio3um             1454 non-null   float64\n",
      " 15  ta1.x              1454 non-null   float64\n",
      " 16  salinity1          1454 non-null   float64\n",
      " 17  temperature_degc   1454 non-null   float64\n",
      " 18  dic                1454 non-null   float64\n",
      "dtypes: float64(17), int64(2)\n",
      "memory usage: 216.0 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Import data\n",
    "train_df = pd.read_csv('data/train.csv')\n",
    "train_df.columns = train_df.columns.str.lower().str.replace(' ', '_')\n",
    "\n",
    "# Data exploration\n",
    "print(train_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eda986be-bb6b-46ce-a61c-73280b2f9fc7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check distribution of outcome variable\n",
    "#sns.histplot(train_df['dic'], kde=False)\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c979c5-4b42-42e2-a9ab-8f45bf357cca",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94680030-8307-4849-ad23-6173599e0393",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Remove 'id' and 'unnamed:_12' columns (for reasons specified above)\n",
    "train_df = train_df.drop(['id', 'unnamed:_12'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4ebd6b-d3ed-4b44-8f8d-9ef78203374d",
   "metadata": {},
   "source": [
    "## Build & train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "7492d066-eab1-43af-8460-dc617b65a352",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Lambda, LeakyReLU\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "f6c6cc63-367a-4db7-8434-278a5f1d9e4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define feature matrix and target vector\n",
    "X_train = train_df.drop('dic', axis=1).values\n",
    "y_train = train_df['dic'].values\n",
    "\n",
    "# Initialize the scalers\n",
    "feature_scaler = StandardScaler()\n",
    "target_scaler = MinMaxScaler()\n",
    "\n",
    "# Scale the features\n",
    "X_train_scaled = feature_scaler.fit_transform(X_train)\n",
    "\n",
    "# Scale the target variable\n",
    "y_train_scaled = target_scaler.fit_transform(y_train.reshape(-1, 1))\n",
    "\n",
    "# Now X_train_scaled and y_train_scaled contain the scaled training data,\n",
    "# and X_val_scaled and y_val_scaled contain the scaled validation data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "3f294805-1cab-4305-94cf-d1896b3b525a",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',  # Monitor the validation loss\n",
    "    min_delta=0,  # Minimum change to qualify as an improvement\n",
    "    patience=10,  # Stop after 10 epochs with no improvement\n",
    "    verbose=1,  # Print messages\n",
    "    mode='min',  # Stop when the quantity monitored has stopped decreasing\n",
    "    restore_best_weights=True  # Restore model weights from the epoch with the best value of the monitored quantity\n",
    ")\n",
    "\n",
    "\n",
    "def create_model(input_shape=X.shape[1], dropout_rate=0.1, learning_rate=0.001, beta_1=0.8):\n",
    "    optimizer = Adam(learning_rate=learning_rate, beta_1=beta_1)\n",
    "    \n",
    "    model = Sequential([\n",
    "        Dense(64, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01), input_shape=input_shape),\n",
    "        LeakyReLU(alpha=0.01),\n",
    "        Dense(64, activation='elu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01)),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(64, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01), input_shape=input_shape),\n",
    "        LeakyReLU(alpha=0.01),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(64, activation='swish', kernel_regularizer=l1_l2(l1=0.01, l2=0.01)),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(64, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01), input_shape=input_shape),\n",
    "        LeakyReLU(alpha=0.01),\n",
    "        Dense(64, activation='elu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01)),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(64, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01), input_shape=input_shape),\n",
    "        LeakyReLU(alpha=0.01),\n",
    "        Dense(64, kernel_regularizer=l1_l2(l1=0.01, l2=0.01)),  # New layer\n",
    "        Lambda(lambda x: x * tf.tanh(tf.math.log(1 + tf.exp(x)))),  # Mish activation\n",
    "        Dense(64, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01), input_shape=input_shape),\n",
    "        LeakyReLU(alpha=0.01),\n",
    "        Dense(1, activation='linear', kernel_regularizer=l1_l2(l1=0.01, l2=0.01))\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# Now create the model with input_shape defined\n",
    "test_model = create_model(input_shape=(X.shape[1],))  # Replace X_train.shape[1] with the number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 692,
   "id": "6e9721b4-1cdd-4f5a-8737-35ed02d376c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',  # Monitor the validation loss\n",
    "    min_delta=0.05,  # Minimum change to qualify as an improvement\n",
    "    patience=10,  # Stop after 10 epochs with no improvement\n",
    "    verbose=1,  # Print messages\n",
    "    mode='min',  # Stop when the quantity monitored has stopped decreasing\n",
    "    restore_best_weights=True  # Restore model weights from the epoch with the best value of the monitored quantity\n",
    ")\n",
    "\n",
    "\n",
    "def create_model(input_shape=X.shape[1], dropout_rate=0.1, learning_rate=0.0001, beta_1=0.8):\n",
    "    optimizer = Adam(learning_rate=learning_rate, beta_1=beta_1)\n",
    "    \n",
    "    model = Sequential([\n",
    "        Dense(64, activation='elu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01), input_shape=input_shape),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(64, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01)),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(1, activation='linear', kernel_regularizer=l1_l2(l1=0.01, l2=0.01))\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# Now create the model with input_shape defined\n",
    "test_model = create_model(input_shape=(X.shape[1],)) # Replace X_train.shape[1] with the number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 693,
   "id": "40cb40c6-406e-4be7-8639-d8fcf58e9b70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assuming X_scaled (input features) and y_scaled (target values) are already defined\n",
    "\n",
    "# Train the model\n",
    "history = test_model.fit(\n",
    "    X_train_scaled,  # Training data features\n",
    "    y_train_scaled,  # Training data target values\n",
    "    epochs=50,  # Number of epochs to train for\n",
    "    batch_size=16,  # Batch size\n",
    "    validation_split=0.5,  # Fraction of the training data to be used as validation data\n",
    "    verbose=0,  # Show progress\n",
    "    callbacks=[early_stopping]  # Include the early stopping callback here\n",
    ")\n",
    "\n",
    "# The 'history' object holds a record of the loss values and metric values during training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 694,
   "id": "4aeece47-630b-4572-bd39-8480e5285811",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5991277694702148"
      ]
     },
     "execution_count": 694,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history['loss'][-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 695,
   "id": "27102fe0-915b-424a-9b7e-35c3d70756b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5750105381011963"
      ]
     },
     "execution_count": 695,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history['val_loss'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "id": "4d7d3f3f-c5ac-4605-b1da-a495a1ffc6c3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "y_scaled_pred = test_model.predict(X_train_scaled)  # Assume X_scaled_val is your scaled validation or test features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 697,
   "id": "1f503c34-ea4e-405a-9c8e-5729fe4be586",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred_original = target_scaler.inverse_transform(y_scaled_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 698,
   "id": "23e643a1-f785-44b9-901e-a886b28c93c3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1429.7196134847188"
      ]
     },
     "execution_count": 698,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming y_val contains the original target values for your validation set\n",
    "mean_squared_error(y_train, y_pred_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b393631-5cc2-4b2d-b278-d752726079ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Later, to transform predictions back to the original scale:\n",
    "# predictions_original_scale = scaler.inverse_transform(predictions_scaled)\n",
    "\n",
    "# Retrieve the index of the best model\n",
    "best_index = grid_result.best_index_\n",
    "\n",
    "# Extract the mean and standard deviation of the MSE for the best model\n",
    "mse = grid_result.cv_results_['mean_test_score'][best_index]\n",
    "std_mse = grid_result.cv_results_['std_test_score'][best_index]\n",
    "\n",
    "# Print the results\n",
    "print(f\"MSE for the best model during CV: {mse}\")\n",
    "print(f\"Standard deviation of MSE for the best model during CV: {std_mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a2600a-dae8-4105-8f5c-1bd406179f33",
   "metadata": {},
   "source": [
    "## Predict testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4590eb9-73b0-4581-a19b-19f4f38e074f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import data\n",
    "test_df = pd.read_csv('data/test.csv')\n",
    "test_df.columns = train_df.columns.str.lower().str.replace(' ', '_')\n",
    "\n",
    "# define feature matrix\n",
    "X_test = test_df.drop('dic', axis=1).values\n",
    "\n",
    "# define target vector\n",
    "y_test = test_df['dic'].values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_test_scaled['dic'] = scaler.fit_transform(X_test[['dic']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794c8239-7fdf-4fe1-abbf-8d5fa9f7d3eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Step 1: Retrieve the best model from grid search\n",
    "best_model = grid_result.best_estimator_\n",
    "\n",
    "# Step 2: Prepare test dataset (Make sure it's prepared in the same way as your training data)\n",
    "# If you've applied any transformations to your training dataset, apply the same here\n",
    "X_test = test_df.drop('dic', axis=1, errors='ignore')  # Assuming 'dic' might not be in your test dataset\n",
    "\n",
    "# Step 3: Use the best model to make predictions on the test dataset\n",
    "predictions = history.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb62904-5714-4c56-8206-3c2d524b66e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import data\n",
    "submission_df = pd.read_csv('data/sample_submission.csv')\n",
    "submission_df.columns = submission_df.columns.str.lower().str.replace(' ', '_')\n",
    "\n",
    "# Assuming 'predictions' is your vector of predicted values\n",
    "submission_df['dic'] = predictions\n",
    "\n",
    "submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed7a257-b0fb-4165-9a41-08331ee69b43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "submission_df.to_csv('linus_submission5.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
