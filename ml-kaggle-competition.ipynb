{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "931e1d19-1ce5-4691-98e3-49b5d0f58988",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "You will use the data contained in the train.csv file to train a model that will predict **dissolved inorganic carbon (DIC)** content in the water samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61266060-25da-43e3-ae3d-18179610ef67",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-20 06:45:56.699444: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-20 06:45:56.808949: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-20 06:45:56.812780: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/R/4.2.2/lib/R/lib:/lib:/usr/local/lib:/usr/lib/x86_64-linux-gnu:/usr/lib/jvm/java-11-openjdk-amd64/lib/server\n",
      "2024-03-20 06:45:56.812793: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2024-03-20 06:45:57.425304: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/R/4.2.2/lib/R/lib:/lib:/usr/local/lib:/usr/lib/x86_64-linux-gnu:/usr/lib/jvm/java-11-openjdk-amd64/lib/server\n",
      "2024-03-20 06:45:57.425376: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/R/4.2.2/lib/R/lib:/lib:/usr/local/lib:/usr/lib/x86_64-linux-gnu:/usr/lib/jvm/java-11-openjdk-amd64/lib/server\n",
      "2024-03-20 06:45:57.425383: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.regularizers import l1_l2\n",
    "from keras.optimizers import Adam\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d7fc6d5-b83e-4218-9ce3-dbdfb88fa3d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the environment variable to change the log level\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # 0 = default, 1 = no INFO, 2 = no INFO and WARNING, 3 = no INFO, WARNING, and ERROR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b400281-4d1f-4dde-9570-02d83b6e2a42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Turn off scientific notation\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "\n",
    "# Set seed\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd63faaa-3be6-4ad9-afb1-f181c2c01bce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1454 entries, 0 to 1453\n",
      "Data columns (total 19 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   id                 1454 non-null   int64  \n",
      " 1   lat_dec            1454 non-null   float64\n",
      " 2   lon_dec            1454 non-null   float64\n",
      " 3   no2um              1454 non-null   float64\n",
      " 4   no3um              1454 non-null   float64\n",
      " 5   nh3um              1454 non-null   float64\n",
      " 6   r_temp             1454 non-null   float64\n",
      " 7   r_depth            1454 non-null   int64  \n",
      " 8   r_sal              1454 non-null   float64\n",
      " 9   r_dynht            1454 non-null   float64\n",
      " 10  r_nuts             1454 non-null   float64\n",
      " 11  r_oxy_micromol.kg  1454 non-null   float64\n",
      " 12  unnamed:_12        0 non-null      float64\n",
      " 13  po4um              1454 non-null   float64\n",
      " 14  sio3um             1454 non-null   float64\n",
      " 15  ta1.x              1454 non-null   float64\n",
      " 16  salinity1          1454 non-null   float64\n",
      " 17  temperature_degc   1454 non-null   float64\n",
      " 18  dic                1454 non-null   float64\n",
      "dtypes: float64(17), int64(2)\n",
      "memory usage: 216.0 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Import data\n",
    "train_df = pd.read_csv('data/train.csv')\n",
    "train_df.columns = train_df.columns.str.lower().str.replace(' ', '_')\n",
    "\n",
    "# Data exploration\n",
    "print(train_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eda986be-bb6b-46ce-a61c-73280b2f9fc7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check distribution of outcome variable\n",
    "#sns.histplot(train_df['dic'], kde=False)\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c979c5-4b42-42e2-a9ab-8f45bf357cca",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94680030-8307-4849-ad23-6173599e0393",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Remove 'id' and 'unnamed:_12' columns (for reasons specified above)\n",
    "train_df = train_df.drop(['id', 'unnamed:_12'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4ebd6b-d3ed-4b44-8f8d-9ef78203374d",
   "metadata": {},
   "source": [
    "## Build & train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6c6cc63-367a-4db7-8434-278a5f1d9e4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define feature matrix\n",
    "X_train = train_df.drop('dic', axis=1).values\n",
    "\n",
    "# define target vector\n",
    "y_train = train_df['dic'].values\n",
    "\n",
    "# Initialize and fit the scaler\n",
    "feature_scaler = StandardScaler()\n",
    "X_scaled = feature_scaler.fit_transform(X_train)\n",
    "\n",
    "# Reshape y\n",
    "y_scaled = y_train.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69033350-797a-4a89-bd96-0c4e4d847e24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_model(optimizer='adam', learn_rate=0.001, momentum=0.0,\n",
    "                 activation='relu', l1_reg=0.01, l2_reg=0.01,\n",
    "                 dropout_rate=0.0, neurons_layer1=64, neurons_layer2=64):\n",
    "    model = Sequential([\n",
    "        Dense(neurons_layer1, activation=activation,\n",
    "              kernel_regularizer=l1_l2(l1=l1_reg, l2=l2_reg)),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(neurons_layer2, activation=activation,\n",
    "              kernel_regularizer=l1_l2(l1=l1_reg, l2=l2_reg)),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(1, kernel_regularizer=l1_l2(l1=l1_reg, l2=l2_reg))  # Output layer\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7be0ba28-c810-49f0-96fb-47197cab846f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Update model wrapper\n",
    "model = KerasRegressor(model=create_model, verbose=0)\n",
    "\n",
    "# Update param_grid with new hyperparameters\n",
    "param_grid = {\n",
    "    'model__momentum': [0.0, 0.8, 0.9, 0.95, 0.99],\n",
    "    'model__activation': ['relu', 'tanh', 'sigmoid', 'linear', 'elu', 'selu'],\n",
    "    'model__dropout_rate': [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5680f9b-7879-401b-a1d2-19d420a8824b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initiate GridSearchCV\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4d59a3-b1aa-46bb-b955-c89fc5c7aac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define feature matrix\n",
    "X_train = train_df.drop('dic', axis=1).values\n",
    "\n",
    "# define target vector\n",
    "y_train = train_df['dic'].values\n",
    "\n",
    "# Initialize and fit the scaler\n",
    "feature_scaler = StandardScaler()\n",
    "X_scaled = feature_scaler.fit_transform(X_train)\n",
    "\n",
    "# Reshape y\n",
    "y_scaled = y_train.reshape(-1, 1)\n",
    "\n",
    "def create_model(optimizer='adam', learn_rate=0.001, momentum=0.0,\n",
    "                 activation='relu', l1_reg=0.01, l2_reg=0.01,\n",
    "                 dropout_rate=0.0, neurons_layer1=64, neurons_layer2=64):\n",
    "    model = Sequential([\n",
    "        Dense(neurons_layer1, activation=activation,\n",
    "              kernel_regularizer=l1_l2(l1=l1_reg, l2=l2_reg)),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(neurons_layer2, activation=activation,\n",
    "              kernel_regularizer=l1_l2(l1=l1_reg, l2=l2_reg)),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(1, kernel_regularizer=l1_l2(l1=l1_reg, l2=l2_reg))  # Output layer\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# Update model wrapper\n",
    "model = KerasRegressor(model=create_model, verbose=0)\n",
    "\n",
    "# Update param_grid with new hyperparameters\n",
    "param_grid = {\n",
    "    'model__momentum': [0.0, 0.8, 0.9, 0.95, 0.99],\n",
    "    'model__activation': ['relu', 'tanh', 'sigmoid', 'linear', 'elu', 'selu'],\n",
    "    'model__dropout_rate': [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "}\n",
    "\n",
    "# Initiate GridSearchCV\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f42c97f-bb3f-477b-b0b8-9a6802777c4c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/python/3.7.13/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
      "1800 fits failed out of a total of 1800.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "1800 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/python/3.7.13/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/linusghanadan/.local/lib/python3.7/site-packages/scikeras/wrappers.py\", line 767, in fit\n",
      "    **kwargs,\n",
      "  File \"/Users/linusghanadan/.local/lib/python3.7/site-packages/scikeras/wrappers.py\", line 929, in _fit\n",
      "    self._check_model_compatibility(y)\n",
      "  File \"/Users/linusghanadan/.local/lib/python3.7/site-packages/scikeras/wrappers.py\", line 551, in _check_model_compatibility\n",
      "    if self.n_outputs_expected_ != len(self.model_.outputs):\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/opt/python/3.7.13/lib/python3.7/site-packages/sklearn/model_selection/_search.py:972: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "  category=UserWarning,\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_727255/1793226968.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train using the features and the normalized outcome\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgrid_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/python/3.7.13/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    924\u001b[0m             \u001b[0mrefit_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 926\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    927\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/scikeras/wrappers.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, **kwargs)\u001b[0m\n\u001b[1;32m    765\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m             \u001b[0mwarm_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarm_start\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 767\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    768\u001b[0m         )\n\u001b[1;32m    769\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/scikeras/wrappers.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, sample_weight, warm_start, epochs, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    927\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_encoder_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 929\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_model_compatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    930\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         self._fit_keras_model(\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/scikeras/wrappers.py\u001b[0m in \u001b[0;36m_check_model_compatibility\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0;31m# we recognize the attribute but do not force it to be\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;31m# generated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_expected_\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m                 raise ValueError(\n\u001b[1;32m    553\u001b[0m                     \u001b[0;34m\"Detected a Keras model input of size\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "# Train using the features and the normalized outcome\n",
    "grid_result = grid.fit(X_scaled, y_scaled, epochs=100, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b393631-5cc2-4b2d-b278-d752726079ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Later, to transform predictions back to the original scale:\n",
    "# predictions_original_scale = scaler.inverse_transform(predictions_scaled)\n",
    "\n",
    "# Retrieve the index of the best model\n",
    "best_index = grid_result.best_index_\n",
    "\n",
    "# Extract the mean and standard deviation of the MSE for the best model\n",
    "mse = grid_result.cv_results_['mean_test_score'][best_index]\n",
    "std_mse = grid_result.cv_results_['std_test_score'][best_index]\n",
    "\n",
    "# Print the results\n",
    "print(f\"MSE for the best model during CV: {mse}\")\n",
    "print(f\"Standard deviation of MSE for the best model during CV: {std_mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a2600a-dae8-4105-8f5c-1bd406179f33",
   "metadata": {},
   "source": [
    "## Predict testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4590eb9-73b0-4581-a19b-19f4f38e074f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import data\n",
    "test_df = pd.read_csv('data/test.csv')\n",
    "test_df.columns = train_df.columns.str.lower().str.replace(' ', '_')\n",
    "\n",
    "# define feature matrix\n",
    "X_test = test_df.drop('dic', axis=1).values\n",
    "\n",
    "# define target vector\n",
    "y_test = test_df['dic'].values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_test_scaled['dic'] = scaler.fit_transform(X_test[['dic']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794c8239-7fdf-4fe1-abbf-8d5fa9f7d3eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Step 1: Retrieve the best model from grid search\n",
    "best_model = grid_result.best_estimator_\n",
    "\n",
    "# Step 2: Prepare test dataset (Make sure it's prepared in the same way as your training data)\n",
    "# If you've applied any transformations to your training dataset, apply the same here\n",
    "X_test = test_df.drop('dic', axis=1, errors='ignore')  # Assuming 'dic' might not be in your test dataset\n",
    "\n",
    "# Step 3: Use the best model to make predictions on the test dataset\n",
    "predictions = history.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb62904-5714-4c56-8206-3c2d524b66e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import data\n",
    "submission_df = pd.read_csv('data/sample_submission.csv')\n",
    "submission_df.columns = submission_df.columns.str.lower().str.replace(' ', '_')\n",
    "\n",
    "# Assuming 'predictions' is your vector of predicted values\n",
    "submission_df['dic'] = predictions\n",
    "\n",
    "submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed7a257-b0fb-4165-9a41-08331ee69b43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "submission_df.to_csv('linus_submission5.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
