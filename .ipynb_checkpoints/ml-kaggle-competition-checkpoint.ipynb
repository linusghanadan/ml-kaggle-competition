{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "931e1d19-1ce5-4691-98e3-49b5d0f58988",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "You will use the data contained in the train.csv file to train a model that will predict **dissolved inorganic carbon (DIC)** content in the water samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61266060-25da-43e3-ae3d-18179610ef67",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-20 05:19:38.996892: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-20 05:19:39.116626: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-20 05:19:39.120687: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/R/4.2.2/lib/R/lib:/lib:/usr/local/lib:/usr/lib/x86_64-linux-gnu:/usr/lib/jvm/java-11-openjdk-amd64/lib/server\n",
      "2024-03-20 05:19:39.120702: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2024-03-20 05:19:39.730638: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/R/4.2.2/lib/R/lib:/lib:/usr/local/lib:/usr/lib/x86_64-linux-gnu:/usr/lib/jvm/java-11-openjdk-amd64/lib/server\n",
      "2024-03-20 05:19:39.730706: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/R/4.2.2/lib/R/lib:/lib:/usr/local/lib:/usr/lib/x86_64-linux-gnu:/usr/lib/jvm/java-11-openjdk-amd64/lib/server\n",
      "2024-03-20 05:19:39.730713: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.regularizers import l1_l2\n",
    "from keras.optimizers import Adam, RMSprop, SGD\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Normalization\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d7fc6d5-b83e-4218-9ce3-dbdfb88fa3d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set the environment variable to change the log level\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # 0 = default, 1 = no INFO, 2 = no INFO and WARNING, 3 = no INFO, WARNING, and ERROR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b400281-4d1f-4dde-9570-02d83b6e2a42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Turn off scientific notation\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "\n",
    "# Set seed\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd63faaa-3be6-4ad9-afb1-f181c2c01bce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1454 entries, 0 to 1453\n",
      "Data columns (total 19 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   id                 1454 non-null   int64  \n",
      " 1   lat_dec            1454 non-null   float64\n",
      " 2   lon_dec            1454 non-null   float64\n",
      " 3   no2um              1454 non-null   float64\n",
      " 4   no3um              1454 non-null   float64\n",
      " 5   nh3um              1454 non-null   float64\n",
      " 6   r_temp             1454 non-null   float64\n",
      " 7   r_depth            1454 non-null   int64  \n",
      " 8   r_sal              1454 non-null   float64\n",
      " 9   r_dynht            1454 non-null   float64\n",
      " 10  r_nuts             1454 non-null   float64\n",
      " 11  r_oxy_micromol.kg  1454 non-null   float64\n",
      " 12  unnamed:_12        0 non-null      float64\n",
      " 13  po4um              1454 non-null   float64\n",
      " 14  sio3um             1454 non-null   float64\n",
      " 15  ta1.x              1454 non-null   float64\n",
      " 16  salinity1          1454 non-null   float64\n",
      " 17  temperature_degc   1454 non-null   float64\n",
      " 18  dic                1454 non-null   float64\n",
      "dtypes: float64(17), int64(2)\n",
      "memory usage: 216.0 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Import data\n",
    "train_df = pd.read_csv('data/train.csv')\n",
    "train_df.columns = train_df.columns.str.lower().str.replace(' ', '_')\n",
    "\n",
    "# Data exploration\n",
    "print(train_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eda986be-bb6b-46ce-a61c-73280b2f9fc7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check distribution of outcome variable\n",
    "#sns.histplot(train_df['dic'], kde=False)\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c979c5-4b42-42e2-a9ab-8f45bf357cca",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94680030-8307-4849-ad23-6173599e0393",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Remove 'id' and 'unnamed:_12' columns (for reasons specified above)\n",
    "train_df = train_df.drop(['id', 'unnamed:_12'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4ebd6b-d3ed-4b44-8f8d-9ef78203374d",
   "metadata": {},
   "source": [
    "## Build & train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aebb429-0300-4d74-b02a-e6ea42ed54d6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/python/3.7.13/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py:703: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    }
   ],
   "source": [
    "def create_model(optimizer='adam', learn_rate=0.001, momentum=0.0,\n",
    "                 activation='relu', l1_reg=0.01, l2_reg=0.01,\n",
    "                 dropout_rate=0.0, neurons_layer1=64, neurons_layer2=64):\n",
    "    # Assuming train_df is available to compute the normalization layer statistics\n",
    "    normalizer = Normalization(axis=-1)\n",
    "    normalizer.adapt(train_df.drop('dic', axis=1).to_numpy())\n",
    "    \n",
    "    model = Sequential([\n",
    "        normalizer,  # Normalization layer for features\n",
    "        Dense(neurons_layer1, activation=activation,\n",
    "              kernel_regularizer=l1_l2(l1=l1_reg, l2=l2_reg)),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(neurons_layer2, activation=activation,\n",
    "              kernel_regularizer=l1_l2(l1=l1_reg, l2=l2_reg)),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(1, kernel_regularizer=l1_l2(l1=l1_reg, l2=l2_reg))  # Output layer\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# Normalize 'dic' using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "train_df['dic_scaled'] = scaler.fit_transform(train_df[['dic']])\n",
    "\n",
    "# Update model wrapper\n",
    "model = KerasRegressor(model=create_model, verbose=0)\n",
    "\n",
    "# Update param_grid with new hyperparameters\n",
    "param_grid = {\n",
    "    'model__momentum': [0.0, 0.8, 0.9, 0.99],\n",
    "    'model__activation': ['relu', 'tanh', 'sigmoid', 'linear', 'elu', 'selu'],\n",
    "    'model__l1_reg': [0.001, 0.01, 0.1],\n",
    "    'model__l2_reg': [0.001, 0.01, 0.1],\n",
    "    'model__dropout_rate': [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "}\n",
    "\n",
    "# Initiate GridSearchCV\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=10)\n",
    "\n",
    "# Train using the features and the normalized outcome\n",
    "grid_result = grid.fit(train_df.drop(['dic', 'dic_scaled'], axis=1), train_df['dic_scaled'], epochs=100, batch_size=16)\n",
    "\n",
    "# Later, to transform predictions back to the original scale:\n",
    "# predictions_original_scale = scaler.inverse_transform(predictions_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b393631-5cc2-4b2d-b278-d752726079ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Retrieve the index of the best model\n",
    "best_index = grid_result.best_index_\n",
    "\n",
    "# Extract the mean and standard deviation of the MSE for the best model\n",
    "mse = grid_result.cv_results_['mean_test_score'][best_index]\n",
    "std_mse = grid_result.cv_results_['std_test_score'][best_index]\n",
    "\n",
    "# Print the results\n",
    "print(f\"MSE for the best model during CV: {mse}\")\n",
    "print(f\"Standard deviation of MSE for the best model during CV: {std_mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a2600a-dae8-4105-8f5c-1bd406179f33",
   "metadata": {},
   "source": [
    "## Predict testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4590eb9-73b0-4581-a19b-19f4f38e074f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import data\n",
    "test_df = pd.read_csv('data/test.csv')\n",
    "test_df.columns = train_df.columns.str.lower().str.replace(' ', '_')\n",
    "\n",
    "# define feature matrix\n",
    "X_test = test_df.drop('dic', axis=1).values\n",
    "\n",
    "# define target vector\n",
    "y_test = test_df['dic'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794c8239-7fdf-4fe1-abbf-8d5fa9f7d3eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Step 1: Retrieve the best model from grid search\n",
    "best_model = grid_result.best_estimator_\n",
    "\n",
    "# Step 2: Prepare test dataset (Make sure it's prepared in the same way as your training data)\n",
    "# If you've applied any transformations to your training dataset, apply the same here\n",
    "X_test = test_df.drop('dic', axis=1, errors='ignore')  # Assuming 'dic' might not be in your test dataset\n",
    "\n",
    "# Step 3: Use the best model to make predictions on the test dataset\n",
    "predictions = history.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb62904-5714-4c56-8206-3c2d524b66e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import data\n",
    "submission_df = pd.read_csv('data/sample_submission.csv')\n",
    "submission_df.columns = submission_df.columns.str.lower().str.replace(' ', '_')\n",
    "\n",
    "# Assuming 'predictions' is your vector of predicted values\n",
    "submission_df['dic'] = predictions\n",
    "\n",
    "submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed7a257-b0fb-4165-9a41-08331ee69b43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "submission_df.to_csv('linus_submission5.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
