{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db8f35d9-c3f3-4afd-acd4-e1e900a37bc6",
   "metadata": {},
   "source": [
    "# ML Kaggle Competition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931e1d19-1ce5-4691-98e3-49b5d0f58988",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "We will use the data contained in the train.csv file to train a model that will predict **dissolved inorganic carbon (DIC)** content in water samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9e527c-a503-4b2d-8057-2337741f2dc0",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61266060-25da-43e3-ae3d-18179610ef67",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-21 21:29:22.885560: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-21 21:29:23.001203: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-21 21:29:23.005084: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/R/4.2.2/lib/R/lib:/lib:/usr/local/lib:/usr/lib/x86_64-linux-gnu:/usr/lib/jvm/java-11-openjdk-amd64/lib/server\n",
      "2024-03-21 21:29:23.005097: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2024-03-21 21:29:23.617327: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/R/4.2.2/lib/R/lib:/lib:/usr/local/lib:/usr/lib/x86_64-linux-gnu:/usr/lib/jvm/java-11-openjdk-amd64/lib/server\n",
      "2024-03-21 21:29:23.617396: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/R/4.2.2/lib/R/lib:/lib:/usr/local/lib:/usr/lib/x86_64-linux-gnu:/usr/lib/jvm/java-11-openjdk-amd64/lib/server\n",
      "2024-03-21 21:29:23.617403: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "# load libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from keras_tuner import HyperModel, RandomSearch\n",
    "from sklearn.model_selection import KFold\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a690f488-f26a-4b8a-bb8b-3a4d1cf0d1b7",
   "metadata": {},
   "source": [
    "## Import & pre-process training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd63faaa-3be6-4ad9-afb1-f181c2c01bce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1454 entries, 0 to 1453\n",
      "Data columns (total 19 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   id                 1454 non-null   int64  \n",
      " 1   lat_dec            1454 non-null   float64\n",
      " 2   lon_dec            1454 non-null   float64\n",
      " 3   no2um              1454 non-null   float64\n",
      " 4   no3um              1454 non-null   float64\n",
      " 5   nh3um              1454 non-null   float64\n",
      " 6   r_temp             1454 non-null   float64\n",
      " 7   r_depth            1454 non-null   int64  \n",
      " 8   r_sal              1454 non-null   float64\n",
      " 9   r_dynht            1454 non-null   float64\n",
      " 10  r_nuts             1454 non-null   float64\n",
      " 11  r_oxy_micromol.kg  1454 non-null   float64\n",
      " 12  unnamed:_12        0 non-null      float64\n",
      " 13  po4um              1454 non-null   float64\n",
      " 14  sio3um             1454 non-null   float64\n",
      " 15  ta1.x              1454 non-null   float64\n",
      " 16  salinity1          1454 non-null   float64\n",
      " 17  temperature_degc   1454 non-null   float64\n",
      " 18  dic                1454 non-null   float64\n",
      "dtypes: float64(17), int64(2)\n",
      "memory usage: 216.0 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# import training data\n",
    "train_df = pd.read_csv('data/train.csv')\n",
    "train_df.columns = train_df.columns.str.lower().str.replace(' ', '_') # clean column names\n",
    "\n",
    "# inspect data\n",
    "print(train_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c979c5-4b42-42e2-a9ab-8f45bf357cca",
   "metadata": {},
   "source": [
    "We will remove column 12 b/c there are 0 non-null values. We will also remove the 'id' column because we don't expect it to be a relevant predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94680030-8307-4849-ad23-6173599e0393",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# remove 'id' and 'unnamed:_12' columns\n",
    "train_df = train_df.drop(['id', 'unnamed:_12'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "687f660a-8d08-48fa-bf2a-bead3ac1fb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define feature matrix for training data\n",
    "X_train = train_df.drop('dic', axis=1).values\n",
    "\n",
    "# define target vector for training data\n",
    "y_train = train_df['dic'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4ebd6b-d3ed-4b44-8f8d-9ef78203374d",
   "metadata": {},
   "source": [
    "## Build & train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d3fec73-7b7b-463e-9e0c-da051438d567",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# initialize new HyperModel object\n",
    "class MyHyperModel:\n",
    "    \n",
    "    def __init__(self, input_shape):\n",
    "        self.input_shape = input_shape # store input shape as an instance attribute\n",
    "\n",
    "    def build(self, hp):\n",
    "        model = Sequential()\n",
    "        \n",
    "        # add dense layer with ReLU (based on preliminary training results)\n",
    "        model.add(Dense(units=hp['neurons_0'], # tune units (number of neurons)\n",
    "                        activation='relu', # select ReLU activator (based on preliminary training results)\n",
    "                        kernel_regularizer=l1_l2(l1=0.01, l2=0.01), # set L1 and L2\n",
    "                        input_shape=self.input_shape)) # specify input shape\n",
    "        \n",
    "        # add dense layer with ELU activator (based on preliminary training results)\n",
    "        model.add(Dense(units=hp['neurons_1'], # tune units (number of neurons)\n",
    "                        activation='elu', # select ELU activator (based on preliminary training results)\n",
    "                        kernel_regularizer=l1_l2(l1=0.01, l2=0.01))) # set L1 and L2\n",
    "        \n",
    "        # add dropout layer\n",
    "        model.add(Dropout(rate=hp['dropout_1'])) # tune dropout rate\n",
    "\n",
    "        # add additional dense layer\n",
    "        for i in range(1, hp['num_layers']):\n",
    "            model.add(Dense(units=hp['neurons_2'], # tune units (number of neurons)\n",
    "                            activation=hp['activation'], # tune activation function\n",
    "                            kernel_regularizer=l1_l2(l1=0.01, l2=0.01))) # set L1 and L2\n",
    "        \n",
    "        # add output layer with linear activation\n",
    "        model.add(Dense(1, activation='linear', kernel_regularizer=l1_l2(l1=0.01, l2=0.01)))\n",
    "        \n",
    "        # configure tuning for optimizer\n",
    "        optimizer = Adam(learning_rate=hp['learning_rate'], beta_1=hp['beta_1'])\n",
    "        \n",
    "        # compile hypermodel and set MSE as loss function\n",
    "        model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "        \n",
    "        return model\n",
    "    \n",
    "# store HyperModel object with specified input shape based on number of columns in feature matrix\n",
    "hypermodel = MyHyperModel(input_shape=X_train.shape[1:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb4a6262-c035-4962-896e-220e9a196b57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create hyperparameter grid for tuning\n",
    "hyperparameter_grid = {\n",
    "    'neurons_0': [64, 128],\n",
    "    'neurons_1': [64, 128],\n",
    "    'dropout_1': [0.0, 0.1, 0.2],\n",
    "    'num_layers': [2],\n",
    "    'neurons_2': [64, 128],\n",
    "    'activation': ['relu', 'elu'],\n",
    "    'learning_rate': [1e-4, 1e-3],\n",
    "    'beta_1': [0.8, 0.9, 0.99]\n",
    "}\n",
    "\n",
    "# define function that creates all combinations of values stored in a dictionary\n",
    "def generate_combinations(grid):\n",
    "    keys, values = zip(*grid.items())\n",
    "    combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "    return combinations\n",
    "\n",
    "# store all combinations of hyperparameter values from grid\n",
    "combinations = generate_combinations(hyperparameter_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "690df2c6-2f27-4b6f-971e-bffe828d17f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create EarlyStopping object to use when tuning hypermodel\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='loss', # monitor loss function\n",
    "    min_delta=0.1, # set minimum decrease in loss function to be read as improvement\n",
    "    patience=10, # stop trial early if no improvement over 10 iterations\n",
    "    verbose=0, # disable verbose\n",
    "    mode='min', # specify that objective is to minimize function being monitored\n",
    "    restore_best_weights=True) # after early stopping, revert model weights to those from the epoch with the best value of the monitored metric\n",
    "\n",
    "# define custom function for performing a CV trial\n",
    "def cross_validate_combination(X, y, combination):\n",
    "    kf = KFold(n_splits=10) # initialize CV fold with 10 splits\n",
    "    val_scores = [] # initialize empty vector for validation scores\n",
    "    \n",
    "    for train_index, val_index in kf.split(X):\n",
    "        \n",
    "        # build model with combination of hyperparameters\n",
    "        model = hypermodel.build(combination)\n",
    "        \n",
    "        # build CV fold (with 10 splits) using all of training data\n",
    "        X_train_fold, X_val_fold = X[train_index], X[val_index]\n",
    "        y_train_fold, y_val_fold = y[train_index], y[val_index]\n",
    "        \n",
    "        # fit model to CV fold\n",
    "        model.fit(X_train_fold,\n",
    "                  y_train_fold,\n",
    "                  callbacks=[early_stopping], # use early stopping\n",
    "                  epochs=50, # set number of epochs for each trial\n",
    "                  verbose=0) # disable verbose\n",
    "        \n",
    "        # evaluate model performance\n",
    "        val_score = model.evaluate(X_val_fold,\n",
    "                                   y_val_fold,\n",
    "                                   verbose=0)\n",
    "        val_scores.append(val_score)\n",
    "    \n",
    "    # return average validation score across all 10 splits of CV fold\n",
    "    return np.mean(val_scores)\n",
    "\n",
    "# initialize objects for storing best CV score and best hyperparameter combination\n",
    "best_score = float('inf')\n",
    "best_combination = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4027bcf4-1d8b-48d3-abfa-c1b13cc58e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-21 21:29:24.648838: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/R/4.2.2/lib/R/lib:/lib:/usr/local/lib:/usr/lib/x86_64-linux-gnu:/usr/lib/jvm/java-11-openjdk-amd64/lib/server\n",
      "2024-03-21 21:29:24.648878: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2024-03-21 21:29:24.648895: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (taylor): /proc/driver/nvidia/version does not exist\n",
      "2024-03-21 21:29:24.649146: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# determine best hyperparameter combination based on CV score\n",
    "for combination in combinations:\n",
    "    score = cross_validate_combination(X_train,\n",
    "                                       y_train,\n",
    "                                       combination)\n",
    "    if score < best_score:\n",
    "        best_score = score\n",
    "        best_combination = combination\n",
    "print(\"Best Hyperparameters:\", best_combination)\n",
    "print(\"Best Score:\", best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a186034e-885f-4e36-bc88-c2ed1a17eccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build version of hypermodel with best combination of hyperparameters\n",
    "best_model = hypermodel(best_combination, input_shape=X_train.shape[1:])\n",
    "\n",
    "# fit model to training data\n",
    "best_model.fit(X_train, y_train, epochs=100, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a2600a-dae8-4105-8f5c-1bd406179f33",
   "metadata": {},
   "source": [
    "## Import & process testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4590eb9-73b0-4581-a19b-19f4f38e074f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import testing data\n",
    "test_df = pd.read_csv('data/test.csv')\n",
    "test_df.columns = train_df.columns.str.lower().str.replace(' ', '_') # clean column names\n",
    "\n",
    "# define feature matrix for testing data\n",
    "X_test = test_df.drop('dic', axis=1).values\n",
    "\n",
    "# remove 'id' and 'unnamed:_12' columns\n",
    "train_df = train_df.drop(['id', 'unnamed:_12'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8da028-8430-46dc-8b44-35e05f7ce1f2",
   "metadata": {},
   "source": [
    "## Predict DIC for testing data & export submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb62904-5714-4c56-8206-3c2d524b66e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# generate predictions for testing data\n",
    "predictions = best_model.predict(X_test)\n",
    "\n",
    "# import submission template\n",
    "submission_df = pd.read_csv('data/sample_submission.csv')\n",
    "submission_df.columns = submission_df.columns.str.lower().str.replace(' ', '_')\n",
    "\n",
    "# bind predictions to 'dic' column\n",
    "submission_df['dic'] = predictions\n",
    "submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed7a257-b0fb-4165-9a41-08331ee69b43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# export submission\n",
    "submission_df.to_csv('linus_submission5.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13",
   "language": "python",
   "name": "py3.7.13"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
